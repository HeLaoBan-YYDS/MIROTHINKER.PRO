{
  "translategemma": {
    "navbar": {
      "logo": "TranslateGemma",
      "slogan": "Next-Gen AI Translation: Deep Context · Open Source · Professional Precision",
      "textTranslation": "Text Translation",
      "imageTranslation": "Image Translation",
      "voiceTranslation": "Voice Translation",
      "documentTranslation": "Document Translation",
      "modelDownload": "Model Download",
      "apiDocs": "API Documentation",
      "pricing": "Pricing",
      "languageSwitch": "Language",
      "signIn": "Sign In",
      "signUp": "Sign Up",
      "startFree": "Start Free"
    },
    "hero": {
      "title": "8192 Tokens Context · Professional-Grade AI Translation Engine",
      "subtitle": "Optimized based on Google Gemma architecture, WMT24++ benchmark error rate reduced by 23.5%, 91% professional terminology accuracy, supports 55+ languages (including low-resource languages) for precise translation, eliminating \"out-of-context\" translation.",
      "inputPlaceholder": "Paste or enter content to translate (within 5000 characters)",
      "targetLanguageLabel": "Target Language",
      "modelSelectorLabel": "Select Model",
      "modelOptions": {
        "4b": "Gemma-4B (Lightweight)",
        "12b": "Gemma-12B (Balanced, Recommended)",
        "27b": "Gemma-27B (Professional)"
      },
      "examplesTitle": "Popular Translation Examples",
      "examples": {
        "medical": "This study adopts a randomized controlled trial to explore the blood pressure-lowering effects of a drug on patients with mild-to-moderate hypertension, with a follow-up period of 12 weeks.",
        "legal": "Both parties shall complete the delivery of target assets within 30 days after the effective date of this agreement. Any party failing to perform on time shall pay a penalty of 0.05% of the total contract amount per day.",
        "ecommerce": "This wireless headphone features 36-hour battery life, active noise cancellation and transparency mode, IPX7 waterproof rating, compatible with both iOS and Android systems."
      },
      "uploadButton": "Upload File to Translate",
      "uploadHint": "Supports image OCR extraction, document layout preservation, can process up to 100 pages",
      "trustBanner": "✅ Open source weights available for download | Data stays within borders | No credit card required, 10,000 free Credits upon registration"
    },
    "modelMatrix": {
      "title": "Model Matrix - Professional Decision Guide",
      "subtitle": "Three-column comparison cards to find the best version for you",
      "models": {
        "4b": {
          "name": "Gemma-4B (Lightweight)",
          "positioning": "Ultimate speed, first choice for daily communication",
          "parameters": "4 billion parameters, 4-bit quantization optimization, 256k super-large vocabulary",
          "scenarios": "Social messaging, web browsing, short video subtitles, simple dialogue commands, mobile app localization",
          "hardware": "Smartphone SoC, low-power edge computing devices, regular PCs",
          "comet": "0.86",
          "terminology": "82%",
          "speed": "Millisecond-level (no pressure on edge devices/low computing power devices)",
          "highlights": "Low computing power requirements, supports secondary fine-tuning, suitable for lightweight scenarios",
          "rating": 4
        },
        "12b": {
          "name": "Gemma-12B (Balanced)",
          "positioning": "Golden balance of quality and speed, preferred for general/commercial use",
          "parameters": "12 billion parameters, KV cache mechanism optimization, multi-layer self-attention",
          "scenarios": "Enterprise website localization, marketing email generation, standard technical documentation, business emails, long-form reading",
          "hardware": "Consumer-grade GPUs (e.g., RTX 4090), medium workstations",
          "comet": "0.91",
          "terminology": "88%",
          "speed": "Very fast (no lag in long text translation)",
          "highlights": "Covers 80% of business scenarios, balances efficiency and cost, open source deployable",
          "rating": 5,
          "recommended": true
        },
        "27b": {
          "name": "Gemma-27B (Professional)",
          "positioning": "Expert in complex contexts, essential for professional scenarios",
          "parameters": "27 billion parameters, massive industry bilingual corpus fine-tuning, SwiGLU activation function",
          "scenarios": "Legal contract review, medical literature translation, academic papers, high-precision literary creation, cross-border e-commerce professional product descriptions",
          "hardware": "Enterprise GPU clusters (e.g., A100/H100), high-performance servers",
          "comet": "0.94",
          "terminology": "91%",
          "speed": "Stable output (precise parsing of complex syntax/metaphors)",
          "highlights": "Built-in industry terminology database, high cultural context adaptability, \"form and spirit\" translation",
          "rating": 5
        }
      },
      "comparisonTable": {
        "positioning": "Positioning",
        "parameters": "Core Parameters",
        "scenarios": "Use Cases",
        "hardware": "Hardware Requirements",
        "comet": "Semantic Consistency (COMET)",
        "terminology": "Terminology Accuracy",
        "speed": "Response Speed",
        "highlights": "Key Highlights",
        "rating": "Rating"
      }
    },
    "coreAdvantages": {
      "title": "Core Advantages",
      "subtitle": "Four core capabilities redefining translation standards",
      "advantages": {
        "precision": {
          "title": "Ultimate Precision · No Context Deviation",
          "description": "8192 Tokens super-large context window, eliminating \"out-of-context\" translation; WMT24++ benchmark error rate reduced by 23.5%, semantic consistency (COMET) reaches 0.94, professional terminology accuracy 91%, far exceeding traditional NMT models (BLEU score 42.1, leading GPT-3.5 by 3.6 points)."
        },
        "multimodal": {
          "title": "Multimodal · Full-Scenario Coverage",
          "description": "Text (real-time translation within 5000 characters), images (OCR precise extraction, preserving original layout), voice (transcription + translation synchronized, 3 credits/minute), documents (PDF/docx batch processing), one-stop solution for daily, commercial, and professional full-scenario translation needs."
        },
        "opensource": {
          "title": "Open Source & Controllable · Data Sovereignty Guarantee",
          "description": "Complete model weights open source (direct download from Hugging Face), supports private deployment (runs within enterprise firewall); differential privacy training + real-time sensitive word filtering, data automatically deleted within 72 hours, not used for model training, core business information absolutely secure."
        },
        "lowResource": {
          "title": "Low-Resource Languages · Breaking Barriers",
          "description": "Through transfer learning technology, leveraging high-resource language grammar structures to assist low-resource language translation, rare languages such as Icelandic and Swahili translation quality improved by 25-30%, helping enterprises expand into emerging markets in Africa and Southeast Asia."
        }
      }
    },
    "useCases": {
      "title": "Use Cases",
      "subtitle": "Six scenarios covering all professional translation needs",
      "cases": {
        "crossBorder": {
          "title": "Cross-Border E-commerce",
          "value": "Using 12B model to batch generate multilingual product detail pages, automatically optimize local search keywords, conversion rate increased by 12%, localization cycle shortened from 5 days to 2 hours, manual review costs reduced by 65%."
        },
        "legal": {
          "title": "Legal Compliance",
          "value": "27B model deeply parses contract terms, highlights terminology correspondences, marks semantic ambiguity points, manual intervention ratio reduced from 80% to 15%, avoiding cross-regional compliance risks."
        },
        "academic": {
          "title": "Academic Research",
          "value": "Precisely processes long complex sentences in English papers, maintains professional terminology consistency, supports literature translation in medical, engineering and other fields, COMET semantic consistency reaches 0.94, facilitating global dissemination of research results."
        },
        "developer": {
          "title": "Developer Integration",
          "value": "Compatible with Hugging Face interface, vLLM fast inference engine, integrated with just a few lines of Python code, supports real-time translation of UI strings and error messages, suitable for software localization needs."
        },
        "content": {
          "title": "Content Going Global",
          "value": "Captures emotional tones of web novels and short dramas, achieves \"form and spirit\" translation, adapts to target market cultural customs, reduces cross-cultural communication barriers, avoiding awkward mistranslations."
        },
        "localization": {
          "title": "Software Localization",
          "value": "4B model integrated into IDE, real-time translation of code comments and UI text, supports low-power device deployment, suitable for mobile app localization, millisecond-level response speed."
        }
      }
    },
    "developer": {
      "title": "Developer Ecosystem",
      "subtitle": "Open Source Resources + Quick Deployment Guide + API Preview",
      "opensource": {
        "title": "Open Source Resources",
        "description": "Open source weights fully available, supports LoRA secondary fine-tuning (adapting to enterprise-specific terminology libraries), building translation ecosystem together.",
        "links": {
          "huggingface": "Hugging Face Repository",
          "github": "GitHub Source Code",
          "weights": "Model Weights Download"
        }
      },
      "deployment": {
        "title": "Quick Deployment Guide",
        "local": {
          "title": "Local Deployment",
          "description": "Ollama one-click run, no complex configuration required",
          "command": "ollama run translategemma:12b"
        },
        "enterprise": {
          "title": "Enterprise Deployment",
          "description": "Supports distributed inference, 4-bit quantization (VRAM usage reduced by 70%, accuracy loss ≤1%), suitable for A100/H100 GPU clusters"
        },
        "edge": {
          "title": "Edge Deployment",
          "description": "4B model supports smartphone SoC, low-power edge computing devices, meeting mobile scenario needs"
        }
      },
      "api": {
        "title": "API Preview (Python Example)",
        "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Load model (12B balanced version)\nmodel_id = \"google/translategemma-12b\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n\n# Translation example (legal text)\ntext = \"Both parties shall complete the delivery of target assets within 30 days after the effective date of this agreement\"\ninputs = tokenizer(f\"Translate to English: {text}\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**inputs, max_new_tokens=200)\ntranslation = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(translation)",
        "hint": "Supports batch translation, async tasks, compatible with vLLM fast inference, view complete API documentation →"
      }
    },
    "faq": {
      "title": "Frequently Asked Questions",
      "subtitle": "Collapsible panels, first 3 high-frequency questions expanded by default",
      "questions": {
        "free": {
          "question": "Is TranslateGemma free?",
          "answer": "Register now and get 10,000 free Credits (1 Credit = 1 character translation), upgrade to paid plan when credits run out; open source models available for free download, local deployment with no additional costs, suitable for individuals and SMEs in initial stages."
        },
        "difference": {
          "question": "How is it different from Google Translate and ChatGPT translation?",
          "answer": "TranslateGemma is a Google model specifically optimized for translation (not a general-purpose LLM), fine-tuned with billions of bilingual corpora + industry data adaptation, focused on core translation tasks; terminology accuracy and context understanding far exceed general models, and supports open source deployment, more secure data, no \"machine translation feel\"."
        },
        "security": {
          "question": "Is my translation data secure?",
          "answer": "Absolutely secure! Online translation data is encrypted in transit, automatically deleted within 72 hours, not used for model training; supports private deployment, data flows entirely within enterprise firewalls, fully compliant with cross-border data regulations, your secrets stay your secrets."
        },
        "languages": {
          "question": "What languages are supported?",
          "answer": "Covers 55+ core languages, including mainstream language pairs such as Chinese-English, Chinese-Japanese, Chinese-German, English-French, as well as low-resource languages such as Icelandic and Swahili, low-resource language translation quality improved by 25-30% compared to baseline models."
        },
        "terminology": {
          "question": "How is professional terminology accuracy guaranteed?",
          "answer": "The model incorporates bilingual corpora from multiple industries including legal, medical, e-commerce, and technology during fine-tuning, supports enterprises uploading their own terminology libraries for LoRA fine-tuning, training dedicated \"brand translators\", ensuring industry terminology accuracy."
        },
        "billing": {
          "question": "What are the Credits billing rules?",
          "answer": "Text translation 1 Credit per character, image translation 2 Credits per image, voice translation 3 Credits per minute, document translation billed by extracted characters; paid plans support tiered discounts, enterprises with daily translation volume exceeding 1 million characters can enjoy custom pricing."
        }
      }
    },
    "footer": {
      "about": {
        "title": "About Us",
        "description": "TranslateGemma is an open source translation model family developed by Google DeepMind based on the Gemma architecture, released in January 2026, focused on providing \"precise, secure, customizable\" global translation solutions, promoting barrier-free cross-cultural communication.",
        "links": {
          "background": "Project Background",
          "team": "Team Introduction"
        }
      },
      "product": {
        "title": "Product Links",
        "modelDownload": "Model Download Center",
        "apiDocs": "API Documentation",
        "whitepaper": "Technical White Paper",
        "discord": "Developer Community (Discord)",
        "changelog": "Changelog",
        "roadmap": "Roadmap"
      },
      "legal": {
        "title": "Legal & Compliance",
        "privacy": "Privacy Policy",
        "terms": "Terms of Service",
        "cookies": "Cookie Settings",
        "ai": "Responsible AI Practices (Bias Elimination, Compliance Filtering)"
      },
      "contact": {
        "title": "Contact & Social",
        "email": "support@translategemma.org",
        "feedback": "Submit Product Suggestions →",
        "github": "GitHub",
        "huggingface": "Hugging Face",
        "discord": "Discord",
        "twitter": "X (Twitter)"
      },
      "copyright": "© 2026 TranslateGemma. All Rights Reserved. Distributed under Google Gemma open source license."
    }
  }
}
